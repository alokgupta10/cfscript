Perfect — your goal is clear now:
You want each module’s pytest command to run independently, regardless of pass/fail, so that:
	•	All tests run every time.
	•	You can fix the failing ones without rerunning the passing ones.
	•	A failing test does not block others from running.

The script I gave already supports this — but here’s a cleaner version focused specifically on your goal, with clear logic and annotations.

⸻

Final Script: Independent pytest Execution Per Module

#!/bin/bash

# Activate virtual environment
source /harness/venv_linux/bin/activate
export PATH="/harness/venv_linux/bin:$PATH"
export PYTHONPATH=$PYTHONPATH:$(pwd)

# Cert path
CERT_PATH="/harness/DHS_CA4_FullBundle.crt"

# Test modules and their test paths
MODULES=("cci_orchestrator" "cci_assessment_prep" "cci_analyzer")
TEST_PATHS=(
  "/harness/cci_orchestrator/tests/unit"
  "/harness/cci_assessment_prep/tests/unit"
  "/harness/cci_analyzer/tests/unit"
)

# Initialize results
declare -A RESULTS

# Set environment variables per module
set_env_vars() {
  case "$1" in
    "cci_orchestrator")
      export IS_NAUTILUS_MOCKED=No
      export IS_AWS_MOCKED=No
      export IMPORTS_ARE_ABSOLUTE=No
      export IS_PYTEST=No
      export IS_MYSQL_MOCKED=No
      export MYSQL_USE_LOCALHOST=No
      export IS_RABBITMQ_MOCKED=No
      export IS_SPLUNK_MOCKED=No
      export IS_CASM_MOCKED=No
      ;;
    "cci_assessment_prep")
      export IS_NAUTILUS_MOCKED=Yes
      export IS_AWS_MOCKED=No
      export IMPORTS_ARE_ABSOLUTE=No
      export IS_PYTEST=No
      export IS_MYSQL_MOCKED=Yes
      export MYSQL_USE_LOCALHOST=No
      export IS_RABBITMQ_MOCKED=No
      export IS_SPLUNK_MOCKED=Yes
      export IS_CASM_MOCKED=No
      ;;
    "cci_analyzer")
      export IS_NAUTILUS_MOCKED=No
      export IS_AWS_MOCKED=Yes
      export IMPORTS_ARE_ABSOLUTE=Yes
      export IS_PYTEST=No
      export IS_MYSQL_MOCKED=No
      export MYSQL_USE_LOCALHOST=Yes
      export IS_RABBITMQ_MOCKED=No
      export IS_SPLUNK_MOCKED=No
      export IS_CASM_MOCKED=No
      ;;
  esac
}

# Run tests per module, log result but continue even on failure
for i in "${!MODULES[@]}"; do
  MODULE="${MODULES[$i]}"
  TEST_PATH="${TEST_PATHS[$i]}"
  
  echo ""
  echo "====================================="
  echo "Running unit tests for: $MODULE"
  echo "====================================="

  set_env_vars "$MODULE"
  export REQUESTS_CA_BUNDLE="$CERT_PATH"

  # Run tests, capture result
  if pytest "$TEST_PATH" -vs -p no:warnings; then
    RESULTS[$MODULE]="PASS"
  else
    RESULTS[$MODULE]="FAIL"
  fi
done

# Summary
echo ""
echo "========== TEST SUMMARY =========="
for mod in "${!RESULTS[@]}"; do
  echo "$mod => ${RESULTS[$mod]}"
done
echo "=================================="

# Optional: exit 1 if anything failed (can be commented out if not needed)
FAILED=0
for result in "${RESULTS[@]}"; do
  [[ "$result" == "FAIL" ]] && FAILED=1
done

exit $FAILED



⸻

What This Script Achieves for You:
	•	Each module runs pytest independently.
	•	Failures do not interrupt other modules’ tests.
	•	You get a full summary of which passed and which failed.
	•	CI/CD can optionally block on failure, but you still see everything in one run.

Let me know if you’d like to:
	•	Log results to a file
	•	Run tests in parallel
	•	Use this in a Harness step with output summary

Happy to help optimize!
